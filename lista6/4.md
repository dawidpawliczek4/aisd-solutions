**1. Intuicja**

Drzewa o zadanej funkcji

$$
h(v)=\text{długość najkrótszej ścieżki od }v\text{ do pustego dziecka}
$$

równoważymy tak, żeby dla każdego węzła $v$ zachodziło

$$
h(\text{lewy}\;v)\;\ge\;h(\text{prawy}\;v).
$$

Dzięki temu ścieżka po wskaźnikach „prawych synów” jest zawsze najkrótsza, a więc jej długość to $O(\log n)$.

Na tej strukturze możemy zbudować operację **merge** (łączenia dwóch kopców) w następujący sposób:

* Wybieramy mniejszy korzeń jako nowy korzeń.
* Rekurencyjnie łączymy prawą poddrzewo tego korzenia z drugim kopcem.
* Po powrocie sprawdzamy warunek $h(\text{lewy})\ge h(\text{prawy})$; jeśli nie jest spełniony — zamieniamy dzieci miejscami.
* Aktualizujemy wartość $h$ w bieżącym węźle.

Pozostałe operacje wykorzystują **merge** jako prymityw:

* **insert(x)** ⟶ zrób nowy węzeł $\{x\}$ i połącz go z dotychczasowym kopcem przez `merge`.
* **deleteMin()** ⟶ usuń korzeń (to minimum), a jego dwa poddrzewa połącz przez `merge`.

Dzięki temu każda pojedyncza operacja to jedno (lub dwa) wywołań `merge`, a koszt `merge` jest proporcjonalny do długości najkrótszej ścieżki do null, czyli $O(\log n)$.

---

**2. Pseudokod**

```plaintext
// Struktura węzła:
//   key  – klucz
//   left, right – wskaźniki na dzieci (mogą być NULL)
//   h    – h(v) = 1 + min( h(left), h(right) ), a dla pustego = 0

FUNCTION merge(H1, H2):
  if H1 = NULL: return H2
  if H2 = NULL: return H1

  // upewniamy się, że H1.key ≤ H2.key
  if H1.key > H2.key:
    swap H1, H2

  // łączymy H1.right z H2
  H1.right ← merge(H1.right, H2)

  // utrzymujemy warunek h(lewy) ≥ h(prawy)
  if h(H1.left) < h(H1.right):
    swap H1.left, H1.right

  // aktualizacja h
  H1.h ← 1 + min( h(H1.left), h(H1.right) )

  return H1

// Wstawianie elementu x:
FUNCTION insert(H, x):
  NEW ← nowy węzeł z key = x, left = right = NULL, h = 1
  return merge(H, NEW)

// Usuwanie minimum:
FUNCTION deleteMin(H):
  // H ≠ NULL
  return merge(H.left, H.right)
```

---

**3. Poprawność i złożoność**

1. **Zachowanie porządku kopca**

   * W `merge` najpierw wybieramy korzeń o mniejszym kluczu → nowy korzeń jest minimum spośród wszystkich kluczy w obu drzewach.
   * Rekurencja łączy pozostałe elementy w sposób analogiczny, więc na każdym kroku porządek heap‐order jest utrzymywany.

2. **Utrzymanie własności $h(\text{lewy})\ge h(\text{prawy})$**

   * Po rekurencyjnym scaleniach dzieci mogą naruszyć tę nierówność.
   * Natychmiast po każdej operacji łączenia sprawdzamy warunek i ewentualnie zamieniamy dzieci miejscami.
   * Następnie poprawnie obliczamy nowe $h$.
   * Indukcyjnie widać, że we wszystkich węzłach warunek jest zachowany.

3. **Złożoność**

   * Niech „prawa ścieżka” w drzewie $H$ ma długość $r$. Wtedy każdy krok rekurencji `merge` schodzi o jeden poziom w prawo → liczba wywołań rekurencyjnych to co najwyżej $r$.
   * Dzięki warunkowi $h(\text{lewy})\ge h(\text{prawy})$, $r\le\log(n+1)$.
   * Każde wywołanie robi stałą liczbę operacji (porównania, swap, aktualizacja $h$).
   * Stąd

     $$
       \text{czas}(\text{merge}) \;=\; O(r) \;=\; O(\log n).
     $$
   * Operacje **insert** i **deleteMin** wykonują 1–2 wywołania `merge`, więc każda z nich także działa w $O(\log n)$.

W ten sposób uzyskaliśmy meldowalne kolejki priorytetowe, w których wszystkie podstawowe operacje wykonują się w oczekiwanym czasie $O(\log n)$.
